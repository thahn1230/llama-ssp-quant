{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lm_eval import evaluator, tasks\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig\n",
    "import torch\n",
    "import argparse\n",
    "import os\n",
    "import json\n",
    "from accelerate import (\n",
    "    init_empty_weights,\n",
    "    infer_auto_device_map,\n",
    "    dispatch_model,\n",
    "    load_checkpoint_in_model,\n",
    ")\n",
    "from accelerate.utils.modeling import get_balanced_memory\n",
    "from awq.utils.parallel import auto_parallel\n",
    "from awq.quantize.pre_quant import run_awq, apply_awq\n",
    "from awq.quantize.quantizer import (\n",
    "    pseudo_quantize_model_weight,\n",
    "    real_quantize_model_weight,\n",
    ")\n",
    "from awq.utils.lm_eval_adaptor import LMEvalAdaptor\n",
    "from awq.utils.utils import simple_dispatch_model\n",
    "from datasets import load_dataset\n",
    "from torch import nn\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, tokenizer):\n",
    "    testenc = load_dataset('wikitext', 'wikitext-2-raw-v1', split='test')\n",
    "    testenc = tokenizer(\"\\n\\n\".join(testenc['text']), return_tensors='pt')\n",
    "\n",
    "    testenc = testenc.input_ids.to(model.device)\n",
    "    nsamples = 10\n",
    "    model = model.eval()\n",
    "\n",
    "    nlls = []\n",
    "    for i in tqdm.tqdm(range(nsamples), desc=\"evaluating...\"):\n",
    "        batch = testenc[:, (i * 2048):((i + 1) * 2048)].to(model.device)\n",
    "        with torch.no_grad():\n",
    "            lm_logits = model(batch).logits\n",
    "        shift_logits = lm_logits[:, :-1, :].contiguous().float()\n",
    "        shift_labels = testenc[:, (i * 2048):((i + 1) * 2048)][:, 1:]\n",
    "        loss_fct = nn.CrossEntropyLoss()\n",
    "        loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "        neg_log_likelihood = loss.float() * 2048\n",
    "        nlls.append(neg_log_likelihood)\n",
    "\n",
    "    return torch.exp(torch.stack(nlls).sum() / (nsamples * 2048))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_size(model: nn.Module, data_width=16, group_size=-1):\n",
    "\n",
    "    if group_size != -1:\n",
    "        data_width += (16 + 4) / group_size\n",
    "\n",
    "    num_elements = 0\n",
    "    for param in model.parameters():\n",
    "        num_elements += param.numel()\n",
    "    return num_elements * data_width\n",
    "\n",
    "Byte = 8\n",
    "KiB = 1024 * Byte\n",
    "MiB = 1024 * KiB\n",
    "GiB = 1024 * MiB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# core quantization method (simulated quantization)\n",
    "def pseudo_quantize_tensor(w, n_bit=4, q_group_size=-1):\n",
    "    org_w_shape = w.shape\n",
    "    if q_group_size > 0:\n",
    "        assert org_w_shape[-1] % q_group_size == 0\n",
    "        w = w.reshape(-1, q_group_size)\n",
    "\n",
    "    assert w.dim() == 2\n",
    "\n",
    "    # Calculate the maximum (\\alpha) and minimum values (\\beta) in the tensor.\n",
    "    max_val = w.amax(dim=1, keepdim=True)\n",
    "    assert max_val.dim() == 2 and max_val.size(0) == w.size(0) and max_val.size(1) == 1\n",
    "    min_val = w.amin(dim=1, keepdim=True)\n",
    "    assert min_val.dim() == 2 and min_val.size(0) == w.size(0) and min_val.size(1) == 1\n",
    "\n",
    "    # Calculate the scale factor and zero point.  (Formula 1 & 2)\n",
    "    max_int = 2 ** n_bit - 1\n",
    "    scales = (max_val - min_val).clamp(min=1e-5) / max_int\n",
    "    assert scales.shape == max_val.shape\n",
    "    zeros = (-torch.round(min_val / scales)).clamp_(0, max_int)\n",
    "    assert scales.shape == min_val.shape\n",
    "\n",
    "    assert torch.isnan(scales).sum() == 0\n",
    "    assert torch.isnan(w).sum() == 0\n",
    "\n",
    "    # Quantize W: Map values in the range [\\beta, \\alpha] to lie within [0, 2^b - 1] (Formula 3)\n",
    "    w = torch.clamp(torch.round(w / scales) + zeros, 0, max_int)\n",
    "    assert w.dim() == 2 and w.size(0) == scales.size(0) and w.size(1) == q_group_size\n",
    "\n",
    "    # Dequantize W (pseudo quantization, the inverse transformation of Formula 3)\n",
    "    w = (w - zeros) * scales\n",
    "    assert w.dim() == 2 and w.size(0) == scales.size(0) and w.size(1) == q_group_size\n",
    "\n",
    "    assert torch.isnan(w).sum() == 0\n",
    "\n",
    "    w = w.reshape(org_w_shape)\n",
    "    return w\n",
    "\n",
    "@torch.no_grad()\n",
    "def pseudo_quantize_model_weight(\n",
    "    model, w_bit, q_group_size,\n",
    "):\n",
    "    for n, m in model.named_modules():\n",
    "        if isinstance(m, nn.Linear):\n",
    "            m.weight.data = pseudo_quantize_tensor(m.weight.data, n_bit=w_bit, q_group_size=q_group_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "awq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
