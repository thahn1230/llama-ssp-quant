# Implementation of speculative sampling as per
# https://arxiv.org/abs/2302.01318
from collections import namedtuple
import torch

from torch import nn
from logging import info, debug, warning, error, critical

from lssp.base import get_temperature_distribution, sample_fn, stream_token_if_required, tokenizer

torch.manual_seed(1339)


def _draft_sample_k(model, input_ids, K, fallback_threshold=None):
    """sample K tokens from the draft model autoregressively
    draft_logits are a (B, K, V) tensor
    inputs_plus_k are a (B, T+K) tensor
    
    If fallback_threshold is set, the draft model will fall back to the target model
    when the confidence of the draft model is below the threshold.
    """
    inputs_plus_k = input_ids
    draft_logits = []
    actual_tokens_generated = 0  # 실제로 생성된 토큰 수
    
    for t in range(K):
        outputs = model(inputs_plus_k)
        next_token_logits = outputs.logits[:, -1, :]
        
        # Fallback 정책: 작은 모델의 최대 확률이 threshold보다 낮으면 생성 중단
        if fallback_threshold is not None:
            # 소프트맥스 적용하여 확률 분포 얻기
            probs = torch.softmax(next_token_logits, dim=-1)
            max_prob = torch.max(probs, dim=-1)[0]
            
            # 최대 확률이 threshold보다 낮으면 생성 중단
            if max_prob < fallback_threshold:
                debug(f"Fallback triggered: max probability {max_prob.item():.4f} < threshold {fallback_threshold}")
                break
        
        next_token_id = sample_fn(next_token_logits)
        inputs_plus_k = torch.cat(
            [inputs_plus_k, next_token_id.unsqueeze(1)],
            dim=1)
        draft_logits.append(next_token_logits)
        actual_tokens_generated += 1
    
    # 최소 하나의 토큰이라도 생성되었는지 확인
    if actual_tokens_generated > 0:
        draft_logits = torch.stack(draft_logits, dim=1)
        return inputs_plus_k, draft_logits, actual_tokens_generated
    else:
        # Fallback으로 인해 토큰이 생성되지 않았을 경우
        return input_ids, None, 0


def _target_sample_from_distribution(target_distribution, draft_distribution):
    distribution = (target_distribution - draft_distribution)
    distribution = torch.max(distribution,
                             torch.zeros_like(distribution))
    
    # NaN 및 inf 값 처리
    distribution = torch.nan_to_num(distribution, nan=0.0, posinf=0.0, neginf=0.0)
    
    # 합이 0인 경우 처리
    sum_dist = distribution.sum(dim=-1, keepdim=True)
    if (sum_dist == 0).any():
        # 확률이 0인 경우 uniform 분포 사용
        mask = (sum_dist == 0).squeeze()
        if mask.sum() > 0:
            uniform_dist = torch.ones_like(distribution) / distribution.size(-1)
            distribution = torch.where(
                sum_dist == 0, 
                uniform_dist,
                distribution / sum_dist.clamp(min=1e-10)
            )
    else:
        distribution = distribution / sum_dist
    
    return torch.multinomial(distribution, num_samples=1).squeeze(-1)


def _ssp_iteration(target_model, draft_model, input_ids, K=4, display=False, 
                  fallback_threshold=None, rollback_threshold=None):
    _, T = input_ids.shape
    
    # sample K tokens from the draft model autoregressively (with fallback policy)
    inputs_plus_k, draft_logits, actual_tokens_generated = _draft_sample_k(
        draft_model, input_ids, K, fallback_threshold)
    
    # 만약 Fallback으로 인해 토큰이 생성되지 않았다면, 타겟 모델로 바로 1개 토큰 생성
    if actual_tokens_generated == 0:
        debug("No tokens generated by draft model due to fallback policy. Using target model directly.")
        target_outputs = target_model(input_ids)
        next_token_id = sample_fn(target_outputs.logits[:, -1, :])
        input_ids = torch.cat([input_ids, next_token_id.unsqueeze(1)], dim=1)
        stream_token_if_required(input_ids, stream=display)
        return input_ids, 0, 0  # 수락된 토큰 0, 실제 Draft가 생성한 토큰 0
    
    debug(f"Possible continuations: {tokenizer.decode(inputs_plus_k[0,T:], skip_special_tokens=True)}")
    
    # get the logits for the same tokens from the target model
    target_outputs = target_model(inputs_plus_k)
    target_logits = target_outputs.logits[:, -actual_tokens_generated-1:, :]
    target_distribution = get_temperature_distribution(target_logits)
    draft_distribution = get_temperature_distribution(draft_logits)
    
    # 데이터 타입 일치 - 모두 float32로 변환하여 비교 문제 해결
    target_distribution = target_distribution.float()
    draft_distribution = draft_distribution.float()
    
    # Rollback 정책 구현: 타겟 모델과 드래프트 모델 간의 분포 거리 계산
    rollback_position = None
    if rollback_threshold is not None:
        for t in range(actual_tokens_generated):
            # Cross Entropy 거리 계산: target 분포와 draft 분포 간
            # 드래프트 모델이 선택한 토큰
            draft_token = inputs_plus_k[0, T+t]
            # 타겟 모델이 해당 위치에서 드래프트 토큰에 할당한 확률
            target_prob = target_distribution[0, t, draft_token]
            # 크로스 엔트로피 거리 계산 (음의 로그 확률)
            distance = -torch.log(target_prob + 1e-10)
            
            if distance > rollback_threshold:
                debug(f"Rollback triggered at position {t}: distance {distance.item():.4f} > threshold {rollback_threshold}")
                rollback_position = t
                break
    
    # 롤백이 발생했다면 롤백 위치부터 다시 시작
    if rollback_position is not None:
        # 롤백 위치까지의 토큰만 수락
        if rollback_position > 0:
            input_ids = torch.cat([input_ids, inputs_plus_k[:, T:T+rollback_position]], dim=1)
            stream_token_if_required(input_ids, stream=display)
        
        # 롤백 위치에서 타겟 모델로 다음 토큰 생성
        rollback_inputs = input_ids
        next_token_id = sample_fn(target_logits[:, rollback_position, :])
        input_ids = torch.cat([input_ids, next_token_id.unsqueeze(1)], dim=1)
        stream_token_if_required(input_ids, stream=display)
        
        return input_ids, rollback_position, actual_tokens_generated
    
    # 롤백이 없다면 기존 accept-reject 로직 수행
    all_accepted = True
    accept_count = 0  # 수락된 토큰 수를 추적하기 위한 변수
    for t in range(1, actual_tokens_generated+1):
        # target과 draft 모델이 같은 경우를 처리하기 위한 특별 로직
        if torch.all(torch.isclose(target_distribution[:1, t-1, :], draft_distribution[:1, t-1, :])):
            # 모델이 동일한 경우 높은 확률(0.95)로 무조건 수락
            sampled_ratios = torch.ones_like(target_distribution[:1, t-1, inputs_plus_k[0, T+t-1]]) * 0.95
        else:
            sampled_ratios = (
                target_distribution[:1, t-1, inputs_plus_k[0, T+t-1]]
                / draft_distribution[:1, t-1, inputs_plus_k[0, T+t-1]]
            )
            # 너무 큰 값 제한
            sampled_ratios = torch.min(sampled_ratios, torch.ones_like(sampled_ratios))
        
        # 0이나 NaN 값 처리
        sampled_ratios = torch.nan_to_num(sampled_ratios, nan=0.9, posinf=0.9)
        sampled_ratios = torch.clamp(sampled_ratios, min=0.1)  # 최소 10% 확률로 수락
        
        rs = torch.rand_like(sampled_ratios)

        if (rs < sampled_ratios).any():  # 토큰이 수락된 경우
            input_ids = torch.cat([input_ids, inputs_plus_k[:, T + t-1].unsqueeze(1)], dim=1)
            stream_token_if_required(input_ids, stream=display)
            accept_count += 1  # 수락된 토큰 수 증가
        else:
            all_accepted = False
            next_token_id = _target_sample_from_distribution(
                target_distribution[:1, t-1, :],
                draft_distribution[:1, t-1, :]
            )
            input_ids = torch.cat([input_ids, next_token_id.unsqueeze(1)], dim=1)
            stream_token_if_required(input_ids, stream=display)
            break

    # if all tokens were accepted, sample a last one
    if all_accepted:
        next_token_id = sample_fn(target_logits[:1, -1, :])
        input_ids = torch.cat([input_ids, next_token_id.unsqueeze(1)], dim=1)
        stream_token_if_required(input_ids, stream=display)

    debug(f"Accepted continuations: {tokenizer.decode(input_ids[0,T:], skip_special_tokens=True)}")
    
    return input_ids, accept_count, actual_tokens_generated  # accept_count와 draft_count 반환


def ssp(target_model, draft_model, min_nb_tokens, input_ids, K=4, display=False,
        fallback_threshold=None, rollback_threshold=None):
    B, T = input_ids.shape
    assert B == 1, "Batch size must be 1, implement the fixes for B > 1"
    accept_tokens = 0
    generated_tokens = 0
    
    while input_ids.shape[1] < T + min_nb_tokens:
        debug(f"Current length: {input_ids.shape[1]}")
        input_ids, accept_count, draft_count = _ssp_iteration(
            target_model, draft_model, input_ids, K, display,
            fallback_threshold=fallback_threshold, 
            rollback_threshold=rollback_threshold
        )
        accept_tokens += accept_count
        generated_tokens += draft_count
    
    return input_ids, accept_tokens, generated_tokens


class FakeModel(nn.Module):
    def __init__(self, vocab_size):
        super().__init__()
        self.vocab_size = vocab_size

    def __call__(self, input_ids):
        # Create fake logits randomly in the range [-1, 1]
        B, T = input_ids.shape
        logits = torch.rand(B, T, self.vocab_size) * 2 - 1
        return namedtuple('Output', ['logits'])(logits)


if __name__ == '__main__':
    # Test the SSP implementation
    vocab_size = 10
    target_model = FakeModel(vocab_size)
    draft_model = FakeModel(vocab_size)
    input_ids = torch.tensor([[1, 2, 3, 4, 5]])
    print(ssp(target_model, draft_model, 10, input_ids))
